{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#Set Working Directory\n",
    "os.chdir(r\"/Users/glinn/Documents/CSCI5622-machine-learning/project/csci5622project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data sets, after splitting into 75,10,15 train, development, test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/50781562/stratified-splitting-of-pandas-dataframe-in-training-validation-and-test-set\n",
    "# this should be 75%, 10%, 15%\n",
    "test_full = pd.read_csv(r\"data/2018-test.csv\").dropna()\n",
    "train_full = pd.read_csv(r\"data/2018-train.csv\").dropna()\n",
    "validate_full = pd.read_csv(r\"data/2018-validate.csv\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We agreed to define success as simply pledged >= Goal, and to drop projects with a duration under 1 day, considered live at the time of acquiring the data, and projects that were cancelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organize Data by deadline, launched, success, and duraction\n",
    "def preprocess_df(df):\n",
    "    df[\"deadline\"] = pd.to_datetime(df[\"deadline\"])\n",
    "    df[\"launched\"] = pd.to_datetime(df[\"launched\"])\n",
    "    #Define Success as pledged>=goal\n",
    "    df[\"success\"] = df[\"pledged\"] >= df[\"goal\"]\n",
    "    df[\"duration\"] = df[\"deadline\"] - df[\"launched\"]\n",
    "    \n",
    "    return df\n",
    "#This drops data with a duration that is too short, live, or cancelled.\n",
    "def clean_df(df):\n",
    "    df = df.drop(df.loc[df[\"duration\"] < datetime.timedelta(days=1)].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"live\"].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"canceled\"].index)\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Now apply these functions to the datasets:\n",
    "test_organized = preprocess_df(test_full)\n",
    "train_organized = preprocess_df(train_full)\n",
    "validate_organized = preprocess_df(validate_full)\n",
    "test = clean_df(test_organized)\n",
    "train = clean_df(train_organized)\n",
    "validate = clean_df(validate_organized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Baseline from training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our baseline, we will learn the percent of success and failures. We will use these probabilites to guess the outcome of each Kickstarter project. That is, we will take the expectation value of the confusion matrix using the probability of each outcome (pass/fail) learned from the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we find the fraction of success and failures in the training data\n",
    "NTrain = len(train.success)\n",
    "PercSuc = len(train.loc[train.success == True])/NTrain\n",
    "PercFail = 1 - PercSuc\n",
    "\n",
    "# This Function will make a Confusion matrix, given a percent success rate learned from the training data\n",
    "def CMBaseline(df,PercentSuccessOfTrain):\n",
    "    PS = PercentSuccessOfTrain #Percent of training data that succeeded\n",
    "    PF = 1 - PS #Percent of training data that failed\n",
    "    NSuccess = len(df.loc[df.success == True]) #Number of successes in the given dataset\n",
    "    NFail = len(df.loc[df.success == False]) #Number of failures in the given dataset\n",
    "    CM = [[r\"Pred\\Acutal\",\"Success\",\"Fail\"],[\"Success\",NSuccess*PS,NFail*PS],[\"Fail\",NSuccess*PF,NFail*PF]] #Expectation value of the Confusion matrix\n",
    "    return CM\n",
    "\n",
    "def Metrics(CM):\n",
    "    CMnp = np.array([[CM[1][1],CM[2][1]],[CM[2][1],CM[2][2]]])\n",
    "    Acc = np.trace(CMnp)/np.sum(CMnp)\n",
    "    TPR = CMnp[0,0]/(CMnp[0,0]+CMnp[1,0])\n",
    "    FNR = 1-TPR\n",
    "    TNR = CMnp[1,1]/(CMnp[0,1]+CMnp[1,1])\n",
    "    FPR = 1-TNR\n",
    "    Precision = CMnp[0,0]/(CMnp[0,0]+CMnp[0,1])\n",
    "    NPV = CMnp[1,1]/(CMnp[1,0]+CMnp[1,1])\n",
    "    F1 = 2*CMnp[0,0]/(2*CMnp[0,0]+CMnp[0,1]+CMnp[1,0])\n",
    "    Metric = {}\n",
    "    Metric[\"Accuracy\"]=Acc\n",
    "    Metric[\"True_Positive_Rate\"] = TPR\n",
    "    Metric[\"False Negative Rate\"] = FNR\n",
    "    Metric[\"True Negative Rate\"] = TNR\n",
    "    Metric[\"False Positive Rate\"] = FPR\n",
    "    Metric[\"Precision\"] = Precision\n",
    "    Metric[\"Negative Predictive Value\"] = NPV\n",
    "    Metric[\"F1\"] = F1\n",
    "    return Metric\n",
    "\n",
    "def ShowCMPercent(CM):\n",
    "    CMnp = np.array([[CM[1][1],CM[2][1]],[CM[2][1],CM[2][2]]])\n",
    "    Norm = np.sum(CMnp,0)\n",
    "    Norm = Norm.reshape(1,2)\n",
    "    CMnp = CMnp/Norm\n",
    "    print(Norm.shape)\n",
    "    CM[1][1] = CMnp[0,0]\n",
    "    CM[1][2] = CMnp[0,1]\n",
    "    CM[2][1] = CMnp[1,0]\n",
    "    CM[2][2] = CMnp[1,1]\n",
    "    return pd.DataFrame(CM)\n",
    "def AccuracyMajority(df):\n",
    "    NFail = len(df.loc[df.success == False])\n",
    "    NData = len(df[\"success\"])\n",
    "    Accuracy = NFail/NData\n",
    "    return Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's create the expectation value of the confusion matrix for the Validation dataset #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.5213595021886327, 'True_Positive_Rate': 0.402446830642193, 'False Negative Rate': 0.597553169357807, 'True Negative Rate': 0.600800024060874, 'False Positive Rate': 0.39919997593912604, 'Precision': 0.402446830642193, 'Negative Predictive Value': 0.600800024060874, 'F1': 0.402446830642193}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Pred\\Acutal</td>\n",
       "      <td>Success</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>5341.68</td>\n",
       "      <td>8039.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Fail</td>\n",
       "      <td>7931.32</td>\n",
       "      <td>11936.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1        2\n",
       "0  Pred\\Acutal  Success     Fail\n",
       "1      Success  5341.68  8039.28\n",
       "2         Fail  7931.32  11936.7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMVal = CMBaseline(validate,PercSuc)\n",
    "MetricVal = Metrics(CMVal)\n",
    "print(MetricVal)\n",
    "pd.DataFrame(CMVal) #This Line shows the CM in traditional form\n",
    "#ShowCMPercent(CMVal) #This shows the CM in percent form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's create the expectation value of the confusion matrix for the Test dataset #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.5173827589338706, 'True_Positive_Rate': 0.402446830642193, 'False Negative Rate': 0.597553169357807, 'True Negative Rate': 0.5952366668666387, 'False Positive Rate': 0.4047633331333613, 'Precision': 0.402446830642193, 'Negative Predictive Value': 0.5952366668666387, 'F1': 0.402446830642193}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Pred\\Acutal</td>\n",
       "      <td>Success</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>8145.93</td>\n",
       "      <td>11979.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Fail</td>\n",
       "      <td>12095.1</td>\n",
       "      <td>17786.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1        2\n",
       "0  Pred\\Acutal  Success     Fail\n",
       "1      Success  8145.93  11979.2\n",
       "2         Fail  12095.1  17786.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMTest = CMBaseline(test,PercSuc)\n",
    "MetricTest = Metrics(CMTest)\n",
    "print(MetricTest)\n",
    "pd.DataFrame(CMTest) #This Line shows the CM in traditional form\n",
    "#ShowCMPercent(CMTest) #This shows the CM in percent form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.597553169357807\n"
     ]
    }
   ],
   "source": [
    "NumSucTrain = len(train.loc[train.success == True])\n",
    "NumTrain = len(train[\"success\"])\n",
    "NumFailTrain = NumTrain - NumSucTrain\n",
    "print(NumFailTrain/NumTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we choose to assign all data as failures for a majority baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a majority vote baseline on the validation data is 0.600800024060874\n",
      "The accuracy of a majority vote baseline on the test data is 0.5952366668666387\n"
     ]
    }
   ],
   "source": [
    "Acc_Val = AccuracyMajority(validate)\n",
    "Acc_Test = AccuracyMajority(test)\n",
    "print(f\"The accuracy of a majority vote baseline on the validation data is {Acc_Val}\")\n",
    "print(f\"The accuracy of a majority vote baseline on the test data is {Acc_Test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
