{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed for DataFrameMapper\n",
    "# %pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# For SVM stuff\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format does basic work to change the format of columns into something we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df):\n",
    "    df[\"deadline\"] = pd.to_datetime(df[\"deadline\"])\n",
    "    df[\"launched\"] = pd.to_datetime(df[\"launched\"])\n",
    "    df[\"success\"] = df[\"pledged\"] >= df[\"goal\"]\n",
    "    df[\"duration\"] = df[\"deadline\"] - df[\"launched\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean removes columns we don't care about. Namely:\n",
    "* When the duration is less than one day\n",
    "* If the project state is 'live'\n",
    "* If the project state is cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Michael's notebook\n",
    "def clean_df(df):\n",
    "    df = df.drop(df.loc[df[\"duration\"] < datetime.timedelta(days=1)].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"live\"].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"canceled\"].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions define the transformations of the columns we care about into the forms we're interested in running actual algorithms on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC2VEC_OUTPUT_LENGTH=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): \n",
    "    tknzr = WhitespaceTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "def get_main_category_encoder(train_clean):\n",
    "    main_category_le = LabelEncoder()\n",
    "    main_category_le.fit(train_clean['main_category'])\n",
    "    return main_category_le\n",
    "\n",
    "def get_category_encoder(train_clean):\n",
    "    category_le = LabelEncoder()\n",
    "    category_le.fit(train_clean['category'])\n",
    "    return category_le\n",
    "\n",
    "def build_doc2vec(names):\n",
    "    tokenized = names.apply(tokenize)\n",
    "    tokenized = list(tokenized)\n",
    "    \n",
    "    # this is fairly important, having a corpus_file instead of in-memory data\n",
    "    # speeds up building the model significantly\n",
    "    # https://github.com/RaRe-Technologies/gensim/issues/2218\n",
    "    with open(\"data/train_names.txt\", \"w\") as f:\n",
    "        for doc in tokenized:\n",
    "            f.write(\" \".join(doc) + \"\\n\")\n",
    "    \n",
    "    # https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    # note that infer_vector is NOT deterministic\n",
    "    # https://github.com/RaRe-Technologies/gensim/issues/447\n",
    "    # and they shouldn't be forced to be determinstic\n",
    "    model = Doc2Vec(corpus_file=\"data/train_names.txt\", vector_size=DOC2VEC_OUTPUT_LENGTH, min_count=1, workers=7)\n",
    "    return model\n",
    "    \n",
    "# expects a series of names\n",
    "def doc2vec_names(names, model):\n",
    "    tokenized = names.apply(tokenize)\n",
    "    tokenized = list(tokenized)\n",
    "    \n",
    "    vectorized = [model.infer_vector(doc) for doc in tokenized]\n",
    "    return vectorized\n",
    "\n",
    "def get_mapper(train_clean):\n",
    "    main_category_le = get_main_category_encoder(train_clean)\n",
    "    category_le = get_category_encoder(train_clean)\n",
    "    \n",
    "    doc2vec_columns = [\"doc2vec_names_%s\" % i for i in range(DOC2VEC_OUTPUT_LENGTH)]\n",
    "    \n",
    "    mapper_list = [\n",
    "        ('main_category', main_category_le),\n",
    "        ('category', category_le),\n",
    "        (['duration'], StandardScaler()),\n",
    "        (['usd_goal_real'], StandardScaler()),\n",
    "        (['launched_month'], OrdinalEncoder()),\n",
    "        (['deadline_month'], OrdinalEncoder()),\n",
    "#         (doc2vec_columns, None),\n",
    "    ]\n",
    "    for column in doc2vec_columns:\n",
    "        mapper_list.append((column, None))\n",
    "\n",
    "    mapper = DataFrameMapper(mapper_list, df_out=True)\n",
    "    return mapper\n",
    "\n",
    "def transform_df(df, mapper, d2v_model, fit=False):\n",
    "    X = df[[\"name\", \"main_category\", \"category\", \"duration\", \"usd_goal_real\"]].copy()\n",
    "    X[\"launched_month\"] = df[\"launched\"].apply(lambda x: x.month)\n",
    "    X[\"deadline_month\"] = df[\"deadline\"].apply(lambda x: x.month)\n",
    "    X[\"duration_seconds\"] = X[\"duration\"].apply(lambda x: x.seconds)\n",
    "    doc2vec_names_array = np.array(doc2vec_names(X[\"name\"], d2v_model))\n",
    "    \n",
    "    for i in range(DOC2VEC_OUTPUT_LENGTH):\n",
    "        colname = \"doc2vec_names_%s\" % i\n",
    "        X[colname] = doc2vec_names_array[:, i]\n",
    "    \n",
    "    if fit:\n",
    "        X_mapped = mapper.fit_transform(X)\n",
    "    else:\n",
    "        X_mapped = mapper.transform(X)\n",
    "        \n",
    "    y = df[\"success\"].copy()\n",
    "    \n",
    "    # it is ridiculous that this is necessary\n",
    "    for col in X_mapped.columns:\n",
    "        if len(col) > 25 and \"doc2vec\" in col:\n",
    "            X_mapped.rename(columns={col: \"doc2vec_names_%s\" % col[-1]}, inplace=True)\n",
    "    \n",
    "    return X_mapped, y, mapper\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train, x_valid, y_valid)\n",
    "def get_train_and_test(train_file=\"data/2018-train.csv\", valid_file=\"data/2018-validate.csv\", test_file=\"data/2018-test.csv\"):\n",
    "    train_full = pd.read_csv(train_file).dropna()\n",
    "    validate_full = pd.read_csv(valid_file).dropna()\n",
    "    test_full = pd.read_csv(test_file).dropna()\n",
    "    \n",
    "    train_format = format_df(train_full)\n",
    "    validate_format = format_df(validate_full)\n",
    "    test_format = format_df(test_full)\n",
    "    \n",
    "    train_clean = clean_df(train_format)\n",
    "    valid_clean = clean_df(validate_format)\n",
    "    test_clean = clean_df(test_format)\n",
    "    \n",
    "    print(\"building d2v model\")\n",
    "    d2v_model = build_doc2vec(train_clean[\"name\"])\n",
    "    \n",
    "    print(\"getting mapper\")\n",
    "    mapper = get_mapper(train_clean)\n",
    "    \n",
    "    print(\"transforming train\")\n",
    "    X_train, y_train, mapper = transform_df(train_clean, mapper, d2v_model, fit=True)\n",
    "    print(\"transforming valid\")\n",
    "    X_valid, y_valid, mapper = transform_df(valid_clean, mapper, d2v_model, fit=False)\n",
    "    print(\"transforming test\")\n",
    "    X_test, y_test, mapper = transform_df(test_clean, mapper, d2v_model, fit=False)\n",
    "    \n",
    "    return (X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building d2v model\n",
      "getting mapper\n",
      "transforming train\n",
      "transforming valid\n",
      "transforming test\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = get_train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"data/preprocess-doc2vec-x-train.csv\", header=True)\n",
    "y_train.to_csv(\"data/preprocess-doc2vec-y-train.csv\", header=True, index=False)\n",
    "X_valid.to_csv(\"data/preprocess-doc2vec-x-valid.csv\", header=True)\n",
    "y_valid.to_csv(\"data/preprocess-doc2vec-y-valid.csv\", header=True, index=False)\n",
    "X_test.to_csv(\"data/preprocess-doc2vec-x-test.csv\", header=True)\n",
    "y_test.to_csv(\"data/preprocess-doc2vec-y-test.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
