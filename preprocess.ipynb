{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed for DataFrameMapper\n",
    "# %pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# For SVM stuff\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format does basic work to change the format of columns into something we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df):\n",
    "    df[\"deadline\"] = pd.to_datetime(df[\"deadline\"])\n",
    "    df[\"launched\"] = pd.to_datetime(df[\"launched\"])\n",
    "    df[\"success\"] = df[\"pledged\"] >= df[\"goal\"]\n",
    "    df[\"duration\"] = df[\"deadline\"] - df[\"launched\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean removes columns we don't care about. Namely:\n",
    "* When the duration is less than one day\n",
    "* If the project state is 'live'\n",
    "* If the project state is cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Michael's notebook\n",
    "def clean_df(df):\n",
    "    df = df.drop(df.loc[df[\"duration\"] < datetime.timedelta(days=1)].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"live\"].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"canceled\"].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions define the transformations of the columns we care about into the forms we're interested in running actual algorithms on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): \n",
    "    tknzr = WhitespaceTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "# TODO: use an embedding instead??\n",
    "def get_count_vectorizer():\n",
    "    nltk.download('stopwords')\n",
    "    en_stopwords = set(stopwords.words(\"english\")) \n",
    "    count_vectorizer = CountVectorizer(stop_words=en_stopwords, analyzer='word', tokenizer=tokenize, min_df=1)\n",
    "    return count_vectorizer\n",
    "\n",
    "def get_main_category_encoder():\n",
    "    main_category_le = LabelEncoder()\n",
    "    main_category_le.fit(train_clean['main_category'])\n",
    "    return main_category_le\n",
    "\n",
    "def get_category_encoder():\n",
    "    category_le = LabelEncoder()\n",
    "    category_le.fit(train_clean['category'])\n",
    "    return category_le\n",
    "\n",
    "def build_doc2vec(names, embedding_length=20):\n",
    "    tokenized = names.apply(tokenize)\n",
    "    tokenized = list(tokenized)\n",
    "    \n",
    "    # this is fairly important, having a corpus_file instead of in-memory data\n",
    "    # speeds up building the model significantly\n",
    "    # https://github.com/RaRe-Technologies/gensim/issues/2218\n",
    "    with open(\"data/train_names.txt\", \"w\") as f:\n",
    "        for doc in tokenized:\n",
    "            f.write(\" \".join(doc) + \"\\n\")\n",
    "    \n",
    "    # https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    # note that infer_vector is NOT deterministic\n",
    "    # https://github.com/RaRe-Technologies/gensim/issues/447\n",
    "    # and they shouldn't be forced to be determinstic\n",
    "    model = Doc2Vec(corpus_file=\"data/train_names.txt\", vector_size=20, min_count=1, workers=7)\n",
    "    return model\n",
    "    \n",
    "# expects a series of names\n",
    "def doc2vec_names(names, model):\n",
    "    tokenized = names.apply(tokenize)\n",
    "    tokenized = list(tokenized)\n",
    "    \n",
    "    vectorized = [model.infer_vector(doc) for doc in tokenized]\n",
    "    return vectorized\n",
    "\n",
    "def get_mapper():\n",
    "    main_category_le = get_main_category_encoder()\n",
    "    category_le = get_category_encoder()\n",
    "    count_vectorizer = get_count_vectorizer()\n",
    "\n",
    "    mapper = DataFrameMapper([\n",
    "        ('name', count_vectorizer),\n",
    "        ('main_category', main_category_le),\n",
    "        ('category', category_le),\n",
    "        (['duration'], StandardScaler()),\n",
    "        (['usd_goal_real'], StandardScaler()),\n",
    "        (['launched_month', 'deadline_month', 'doc2vec_names'], OrdinalEncoder()),\n",
    "    ], df_out=True)\n",
    "    return mapper\n",
    "\n",
    "def transform_df(df, mapper, d2v_model, fit=False):\n",
    "    X = df[[\"name\", \"main_category\", \"category\", \"duration\", \"usd_goal_real\"]].copy()\n",
    "    X[\"launched_month\"] = df[\"launched\"].apply(lambda x: x.month)\n",
    "    X[\"deadline_month\"] = df[\"deadline\"].apply(lambda x: x.month)\n",
    "    X[\"duration_seconds\"] = X[\"duration\"].apply(lambda x: x.seconds)\n",
    "    X[\"doc2vec_names\"] = doc2vec_names(X[\"name\"], d2v_model)\n",
    "    \n",
    "    if fit:\n",
    "        X_mapped = mapper.fit_transform(X)\n",
    "    else:\n",
    "        X_mapped = mapper.transform(X)\n",
    "        \n",
    "    y = df[\"success\"].copy()\n",
    "    \n",
    "    return X_mapped, y, mapper\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Michael's notebook\n",
    "train_full = pd.read_csv(\"data/2018-train.csv\").dropna()\n",
    "validate_full = pd.read_csv(\"data/2018-validate.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Michael's notebook\n",
    "train_format = format_df(train_full)\n",
    "validate_format = format_df(validate_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Michael's notebook\n",
    "train_clean = clean_df(train_format)\n",
    "valid_clean = clean_df(validate_format)\n",
    "train_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d2v_model = build_doc2vec(train_clean[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = get_mapper()\n",
    "X_train, y_train, mapper = transform_df(train_clean, mapper, d2v_model, fit=True)\n",
    "X_valid, y_valid, mapper = transform_df(valid_clean, mapper, d2v_model, fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: write to file\n",
    "\n",
    "X_train.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
