{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn-pandas in /Users/erika/.local/lib/python3.7/site-packages (1.8.0)\n",
      "Requirement already satisfied: pandas>=0.11.0 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from sklearn-pandas) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from sklearn-pandas) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from sklearn-pandas) (1.3.1)\n",
      "Requirement already satisfied: scikit-learn>=0.15.0 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from sklearn-pandas) (0.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.11.0->sklearn-pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from pandas>=0.11.0->sklearn-pandas) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn>=0.15.0->sklearn-pandas) (0.13.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.11.0->sklearn-pandas) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# This is needed for DataFrameMapper\n",
    "%pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# For SVM stuff\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format does basic work to change the format of columns into something we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df):\n",
    "    df[\"deadline\"] = pd.to_datetime(df[\"deadline\"])\n",
    "    df[\"launched\"] = pd.to_datetime(df[\"launched\"])\n",
    "    df[\"success\"] = df[\"pledged\"] >= df[\"goal\"]\n",
    "    df[\"duration\"] = df[\"deadline\"] - df[\"launched\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean removes columns we don't care about. Namely:\n",
    "* When the duration is less than one day\n",
    "* If the project state is 'live'\n",
    "* If the project state is cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Michael's notebook\n",
    "def clean_df(df):\n",
    "    df = df.drop(df.loc[df[\"duration\"] < datetime.timedelta(days=1)].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"live\"].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"canceled\"].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions define the transformations of the columns we care about into the forms we're interested in running actual algorithms on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): \n",
    "    tknzr = WhitespaceTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "# TODO: use an embedding instead??\n",
    "def get_count_vectorizer():\n",
    "    nltk.download('stopwords')\n",
    "    en_stopwords = set(stopwords.words(\"english\")) \n",
    "    count_vectorizer = CountVectorizer(stop_words=en_stopwords, analyzer='word', tokenizer=tokenize, min_df=1)\n",
    "    return count_vectorizer\n",
    "\n",
    "def get_main_category_encoder():\n",
    "    main_category_le = LabelEncoder()\n",
    "    main_category_le.fit(train_clean['main_category'])\n",
    "    return main_category_le\n",
    "\n",
    "def get_category_encoder():\n",
    "    category_le = LabelEncoder()\n",
    "    category_le.fit(train_clean['category'])\n",
    "    return category_le\n",
    "\n",
    "def build_doc2vec(names, embedding_length=20):\n",
    "    tokenized = names.apply(tokenize)\n",
    "    tokenized = list(tokenized)\n",
    "    \n",
    "    # this is fairly important, having a corpus_file instead of in-memory data\n",
    "    # speeds up building the model significantly\n",
    "    # https://github.com/RaRe-Technologies/gensim/issues/2218\n",
    "    with open(\"data/train_names.txt\", \"w\") as f:\n",
    "        for doc in tokenized:\n",
    "            f.write(\" \".join(doc) + \"\\n\")\n",
    "    \n",
    "    # https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    # note that infer_vector is NOT deterministic\n",
    "    # https://github.com/RaRe-Technologies/gensim/issues/447\n",
    "    # and they shouldn't be forced to be determinstic\n",
    "    model = Doc2Vec(corpus_file=\"data/train_names.txt\", vector_size=20, min_count=1, workers=7)\n",
    "    return model\n",
    "    \n",
    "# expects a series of names\n",
    "def doc2vec_names(names, model):\n",
    "    tokenized = names.apply(tokenize)\n",
    "    tokenized = list(tokenized)\n",
    "    \n",
    "    vectorized = [model.infer_vector(doc) for doc in tokenized]\n",
    "    return vectorized\n",
    "\n",
    "def get_mapper():\n",
    "    main_category_le = get_main_category_encoder()\n",
    "    category_le = get_category_encoder()\n",
    "    count_vectorizer = get_count_vectorizer()\n",
    "\n",
    "    mapper = DataFrameMapper([\n",
    "        ('name', count_vectorizer),\n",
    "        ('main_category', main_category_le),\n",
    "        ('category', category_le),\n",
    "        (['duration'], StandardScaler()),\n",
    "        (['usd_goal_real'], StandardScaler()),\n",
    "        (['launched_month', 'deadline_month'], OrdinalEncoder()),\n",
    "    ], df_out=True)\n",
    "    return mapper\n",
    "\n",
    "def transform_df(df, mapper, fit=False):\n",
    "    X = df[[\"name\", \"main_category\", \"category\", \"duration\", \"usd_goal_real\"]].copy()\n",
    "    X[\"launched_month\"] = df[\"launched\"].apply(lambda x: x.month)\n",
    "    X[\"deadline_month\"] = df[\"deadline\"].apply(lambda x: x.month)\n",
    "    X[\"duration\"] = X[\"duration\"].apply(lambda x: x.seconds)\n",
    "    \n",
    "    if fit:\n",
    "        X_mapped = mapper.fit_transform(X)\n",
    "    else:\n",
    "        X_mapped = mapper.transform(X)\n",
    "        \n",
    "    y = df[\"success\"].copy()\n",
    "    \n",
    "    return X_mapped, y, mapper\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Michael's notebook\n",
    "train_full = pd.read_csv(\"data/2018-train.csv\").dropna()\n",
    "validate_full = pd.read_csv(\"data/2018-validate.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Michael's notebook\n",
    "train_format = format_df(train_full)\n",
    "validate_format = format_df(validate_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd pledged</th>\n",
       "      <th>usd_pledged_real</th>\n",
       "      <th>usd_goal_real</th>\n",
       "      <th>success</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>319122</td>\n",
       "      <td>695425648</td>\n",
       "      <td>Peace-building through story-making with youth...</td>\n",
       "      <td>Children's Books</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>SEK</td>\n",
       "      <td>2015-06-09</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>2015-05-10 14:53:53</td>\n",
       "      <td>6251.0</td>\n",
       "      <td>successful</td>\n",
       "      <td>15</td>\n",
       "      <td>SE</td>\n",
       "      <td>756.92</td>\n",
       "      <td>762.02</td>\n",
       "      <td>731.42</td>\n",
       "      <td>True</td>\n",
       "      <td>29 days 09:06:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>175494</td>\n",
       "      <td>189251239</td>\n",
       "      <td>Colored Baggies for Boardgames</td>\n",
       "      <td>Tabletop Games</td>\n",
       "      <td>Games</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>2012-11-08 20:06:31</td>\n",
       "      <td>15151.0</td>\n",
       "      <td>successful</td>\n",
       "      <td>518</td>\n",
       "      <td>US</td>\n",
       "      <td>15151.00</td>\n",
       "      <td>15151.00</td>\n",
       "      <td>6000.00</td>\n",
       "      <td>True</td>\n",
       "      <td>59 days 03:53:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>141771</td>\n",
       "      <td>1720248225</td>\n",
       "      <td>Two Scoops of Beauty health wellness women eve...</td>\n",
       "      <td>Events</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>5202.0</td>\n",
       "      <td>2016-08-05 01:26:56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5202.00</td>\n",
       "      <td>False</td>\n",
       "      <td>29 days 22:33:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>236644</td>\n",
       "      <td>272982453</td>\n",
       "      <td>Gavarcia - Haute Couture in Canada</td>\n",
       "      <td>Couture</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>CAD</td>\n",
       "      <td>2015-04-16</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2015-03-18 16:27:44</td>\n",
       "      <td>556.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>8</td>\n",
       "      <td>CA</td>\n",
       "      <td>434.86</td>\n",
       "      <td>456.07</td>\n",
       "      <td>2460.83</td>\n",
       "      <td>False</td>\n",
       "      <td>28 days 07:32:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>231360</td>\n",
       "      <td>245912004</td>\n",
       "      <td>\"One Last Crazy F*cking Night\" (#OLCFN) The Movie</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2013-01-30 17:55:02</td>\n",
       "      <td>3041.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>28</td>\n",
       "      <td>US</td>\n",
       "      <td>3041.00</td>\n",
       "      <td>3041.00</td>\n",
       "      <td>50000.00</td>\n",
       "      <td>False</td>\n",
       "      <td>29 days 06:04:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          ID                                               name  \\\n",
       "0      319122   695425648  Peace-building through story-making with youth...   \n",
       "2      175494   189251239                     Colored Baggies for Boardgames   \n",
       "4      141771  1720248225  Two Scoops of Beauty health wellness women eve...   \n",
       "5      236644   272982453                 Gavarcia - Haute Couture in Canada   \n",
       "6      231360   245912004  \"One Last Crazy F*cking Night\" (#OLCFN) The Movie   \n",
       "\n",
       "           category main_category currency   deadline     goal  \\\n",
       "0  Children's Books    Publishing      SEK 2015-06-09   6000.0   \n",
       "2    Tabletop Games         Games      USD 2013-01-07   6000.0   \n",
       "4            Events          Food      USD 2016-09-04   5202.0   \n",
       "5           Couture       Fashion      CAD 2015-04-16   3000.0   \n",
       "6    Narrative Film  Film & Video      USD 2013-03-01  50000.0   \n",
       "\n",
       "             launched  pledged       state  backers country  usd pledged  \\\n",
       "0 2015-05-10 14:53:53   6251.0  successful       15      SE       756.92   \n",
       "2 2012-11-08 20:06:31  15151.0  successful      518      US     15151.00   \n",
       "4 2016-08-05 01:26:56      0.0      failed        0      US         0.00   \n",
       "5 2015-03-18 16:27:44    556.0      failed        8      CA       434.86   \n",
       "6 2013-01-30 17:55:02   3041.0      failed       28      US      3041.00   \n",
       "\n",
       "   usd_pledged_real  usd_goal_real  success         duration  \n",
       "0            762.02         731.42     True 29 days 09:06:07  \n",
       "2          15151.00        6000.00     True 59 days 03:53:29  \n",
       "4              0.00        5202.00    False 29 days 22:33:04  \n",
       "5            456.07        2460.83    False 28 days 07:32:16  \n",
       "6           3041.00       50000.00    False 29 days 06:04:58  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taken from Michael's notebook\n",
    "train_clean = clean_df(train_format)\n",
    "valid_clean = clean_df(validate_format)\n",
    "train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 44s, sys: 49.3 s, total: 4min 34s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# example doc2vec usage\n",
    "model = build_doc2vec(train_clean[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 0 ns, total: 3min 9s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorized_names = doc2vec_names(train_clean[\"name\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.04361228, -0.02603678,  0.04302235, -0.01056258, -0.02098303,\n",
       "        -0.01242309, -0.05192544,  0.0046178 ,  0.01002172, -0.00840567,\n",
       "         0.05564684, -0.09543984, -0.01014506,  0.02251204,  0.03464296,\n",
       "        -0.01593683,  0.00240719,  0.02585901, -0.07216214,  0.03373416],\n",
       "       dtype=float32),\n",
       " array([-0.06794597, -0.01340604,  0.01937855, -0.03883448,  0.02605217,\n",
       "         0.00689759, -0.06503404,  0.00525253,  0.01287744,  0.02599726,\n",
       "        -0.01666297, -0.03203052, -0.0140506 ,  0.05501854,  0.0406184 ,\n",
       "         0.05119792,  0.0056717 , -0.02346437, -0.03816985,  0.01163796],\n",
       "       dtype=float32),\n",
       " array([-0.03060074, -0.01253008, -0.05797678, -0.01393101,  0.00424226,\n",
       "         0.0066396 , -0.02697883,  0.01528488,  0.08123344, -0.02655926,\n",
       "         0.02522497, -0.05560829, -0.0155972 ,  0.09196401,  0.02334286,\n",
       "        -0.12063598, -0.05654754, -0.08536929, -0.112646  ,  0.01996975],\n",
       "       dtype=float32),\n",
       " array([ 0.03833615,  0.01111039,  0.00976846, -0.0313745 ,  0.00851211,\n",
       "        -0.0096732 , -0.05135198,  0.02559441, -0.01940942,  0.00401101,\n",
       "         0.00785356, -0.02674869, -0.00704594,  0.02320244,  0.02124848,\n",
       "        -0.00032372,  0.01867504, -0.00901272, -0.05488476,  0.0144384 ],\n",
       "       dtype=float32),\n",
       " array([-0.00879845, -0.00816504, -0.01022847,  0.0285371 ,  0.00584871,\n",
       "         0.00150441, -0.00014985,  0.00152951,  0.05405761,  0.04203488,\n",
       "        -0.02260674, -0.01868184, -0.04201433,  0.09270095,  0.05246159,\n",
       "         0.06669457,  0.05203013, -0.01731583, -0.03061273,  0.02374027],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/erika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "mapper = get_mapper()\n",
    "X_train, y_train, mapper = transform_df(train_clean, mapper, fit=True)\n",
    "X_valid, y_valid, mapper = transform_df(valid_clean, mapper, fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: write to file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
