{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/glinn/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "import re\n",
    "\n",
    "#Set Working Directory\n",
    "os.chdir(r\"/Users/glinn/Documents/CSCI5622-machine-learning/project/csci5622project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data sets, after splitting into 75,10,15 train, development, test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/50781562/stratified-splitting-of-pandas-dataframe-in-training-validation-and-test-set\n",
    "# this should be 75%, 10%, 15%\n",
    "test_full = pd.read_csv(r\"data/2018-test.csv\").dropna()\n",
    "train_full = pd.read_csv(r\"data/2018-train.csv\").dropna()\n",
    "validate_full = pd.read_csv(r\"data/2018-validate.csv\").dropna()# Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We agreed to define success as simply pledged >= Goal, and to drop projects with a duration under 1 day, considered live at the time of acquiring the data, and projects that were cancelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organize Data by deadline, launched, success, and duraction\n",
    "def preprocess_df(df):\n",
    "    df[\"deadline\"] = pd.to_datetime(df[\"deadline\"])\n",
    "    df[\"launched\"] = pd.to_datetime(df[\"launched\"])\n",
    "    #Define Success as pledged>=goal\n",
    "    df[\"success\"] = df[\"pledged\"] >= df[\"goal\"]\n",
    "    df[\"duration\"] = df[\"deadline\"] - df[\"launched\"]\n",
    "    \n",
    "    return df\n",
    "#This drops data with a duration that is too short, live, or cancelled.\n",
    "def clean_df(df):\n",
    "    df = df.drop(df.loc[df[\"duration\"] < datetime.timedelta(days=1)].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"live\"].index)\n",
    "    df = df.drop(df.loc[df[\"state\"] == \"canceled\"].index)\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Now apply these functions to the datasets:\n",
    "test_organized = preprocess_df(test_full)\n",
    "train_organized = preprocess_df(train_full)\n",
    "validate_organized = preprocess_df(validate_full)\n",
    "test = clean_df(test_organized)\n",
    "train = clean_df(train_organized)\n",
    "validate = clean_df(validate_organized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We want a function that turns the name into basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordsList(Text):\n",
    "    wordList = re.sub(\"[^\\w]\", \" \",  Text).split()\n",
    "    return wordList\n",
    "\n",
    "def unusual_words(text):\n",
    "    Words = WordsList(text)\n",
    "    text_vocab = set(w.lower() for w in Words if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab - english_vocab\n",
    "    return len(sorted(unusual))\n",
    "\n",
    "def ExtractFeatures(Name):\n",
    "    NumChars = len(Name)\n",
    "    NumWeirdChars = sum(not c.isalnum() for c in Name) - sum(c.isspace() for c in Name)\n",
    "    NumUnusualWords = unusual_words(Name)\n",
    "    return NumChars, NumWeirdChars,NumUnusualWords\n",
    "\n",
    "def AddFeaturesTodf(df):\n",
    "    NumElements = len(df[\"name\"])\n",
    "    print(NumElements)\n",
    "    NumCharsList = []\n",
    "    NumWeirdCharsList = []\n",
    "    NumUnusualWordsList = []\n",
    "    count=0\n",
    "    items=0\n",
    "    while items<NumElements:\n",
    "        #This try simply skips over empty name data points\n",
    "        try:\n",
    "            NumChars, NumWeirdChars,NumUnusualWords = ExtractFeatures(df[\"name\"][items+count])\n",
    "            NumCharsList.append(NumChars)\n",
    "            NumWeirdCharsList.append(NumWeirdChars)\n",
    "            NumUnusualWordsList.append(NumUnusualWords)\n",
    "            items+=1\n",
    "        except:\n",
    "            count+=1\n",
    "    df[\"number of characters\"] = NumCharsList\n",
    "    df[\"number of weird characters\"] = NumWeirdCharsList\n",
    "    df[\"number of unusual words\"] = NumUnusualWordsList\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33249\n"
     ]
    }
   ],
   "source": [
    "validate_BasicTitleFeatures = AddFeaturesTodf(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate[\"Number of characters\"] = validate[\"name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_BasicTitleFeatures = AddFeaturesTodf(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_BasicTitleFeatures = AddFeaturesTodf(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_BasicTitleFeatures.to_csv(\"data/2018-validate_BasicTitleFeatures.csv\")\n",
    "train_BasicTitleFeatures.to_csv(\"data/2018-train_BasicTitleFeatures.csv\")\n",
    "test_BasicTitleFeatures.to_csv(\"data/2018-test_BasicTitleFeatures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0          ID                                      name  \\\n",
      "0      188874  1961049069                               Coming Home   \n",
      "1      121497  1617129999  japanesepaper.ca , washi at a fair price   \n",
      "2      314415   671311286       Tifany Lee & Co. Recording Campaign   \n",
      "\n",
      "         category main_category currency   deadline     goal  \\\n",
      "0       Video Art           Art      USD 2015-05-18  15000.0   \n",
      "1          Crafts        Crafts      CAD 2015-04-15    100.0   \n",
      "2  Country & Folk         Music      USD 2011-07-22   2695.0   \n",
      "\n",
      "             launched  pledged  ... backers  country usd pledged  \\\n",
      "0 2015-04-18 19:46:11      0.0  ...       0       US        0.00   \n",
      "1 2015-02-14 23:25:36    263.0  ...       5       CA      210.03   \n",
      "2 2011-06-22 23:43:48   2710.0  ...      20       US     2710.00   \n",
      "\n",
      "   usd_pledged_real  usd_goal_real  success         duration  \\\n",
      "0              0.00       15000.00    False 29 days 04:13:49   \n",
      "1            215.73          82.03     True 59 days 00:34:24   \n",
      "2           2710.00        2695.00     True 29 days 00:16:12   \n",
      "\n",
      "  number of characters  number of weird characters  number of unusual words  \n",
      "0                   11                           0                        0  \n",
      "1                   40                           2                        2  \n",
      "2                   35                           2                        2  \n",
      "\n",
      "[3 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(validate_BasicTitleFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 Coming Home\n",
      "1    japanesepaper.ca , washi at a fair price\n",
      "2         Tifany Lee & Co. Recording Campaign\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(validate.iloc[0:3][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(ExtractFeatures(validate[\"name\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        11\n",
      "1        40\n",
      "2        35\n",
      "3        23\n",
      "4         9\n",
      "         ..\n",
      "37860    24\n",
      "37861    49\n",
      "37862    27\n",
      "37864    39\n",
      "37865    50\n",
      "Name: Number of characters, Length: 33249, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(validate[\"Number of characters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
